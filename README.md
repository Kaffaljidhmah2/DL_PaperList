# Paper List : Deep Learning Modern Techniques

*Preliminary draft. Not for wide circulation.*

---

## Part I : Network Layer

<a href="http://arxiv.org/abs/1412.6071">Fractional MaxPooling</a>

<a href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a>

<a href="https://arxiv.org/abs/1207.0580">Improving neural networks by preventing co-adaptation of feature detectors</a>

<a href="http://arxiv.org/abs/1411.4280">Efficient Object Localization Using Convolutional Networks</a>

<a href="https://arxiv.org/abs/1609.05158">Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</a>

<a href="https://arxiv.org/abs/1602.07868">Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks</a>

<a href="https://arxiv.org/abs/1612.08083">Language Modeling with Gated Convolutional Networks</a>

---

## Part II : Optimization

<a href="https://arxiv.org/abs/1212.5701">ADADELTA: An Adaptive Learning Rate Method</a>

<a href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a>

<a href="https://arxiv.org/pdf/1308.0850v5.pdf">Generating Sequences
With Recurrent Neural Networks</a>

<a href="https://arxiv.org/abs/1608.03983">SGDR: Stochastic Gradient Descent with Warm Restarts</a>

---

** Reference ** : <a href="http://pytorch.org/docs">http://pytorch.org/docs</a>